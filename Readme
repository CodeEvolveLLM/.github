# CodeEvolveLLM [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)

**RL-Driven Iterative Code Generation & Optimization**

Fine-tunes local LLMs (Qwen2.5-coder-7B) for autonomous code generation, debugging, and optimization via reinforcement learning.

**[Repository](https://github.com/sanskar9999/CodeEvolveLLM)** | **Demo:** Coming Soon

## Overview

CodeEvolveLLM: Local LLM + Automated Execution + RL Fine-tuning + Iterative Debugging. Self-improving code, runs on consumer hardware.

**Components:**

*   Code Generation: Qwen2.5-coder-7B.
*   Execution: Python + Docker (sandboxed).
*   RL Fine-tuning: Correctness & efficiency-based rewards.
*   Debugging: Automated feedback loops.

## Features

*   Autonomous Generation: Initial code solutions.
*   Self-Debugging: Error detection & refinement.
*   RL Training: Fine-tuning via successful trajectories.
*   Local Execution: Consumer hardware (7B model).
*   CLI: Simple command-line interaction.
*   Sandboxing: Docker for safe execution.

## Installation

```bash
git clone https://github.com/sanskar9999/CodeEvolveLLM
cd CodeEvolveLLM
pip install -r requirements.txt
python scripts/download_model.py  # Fetches Qwen2.5-coder-7B
Use code with caution.
Markdown
Usage
Code Generation
python main.py --problem "Find the longest substring without repeating characters"
Use code with caution.
Bash
Full Training
Dataset Generation:

python generate_dataset.py --problems 1000 --difficulty medium  # Creates dataset (saved as generated_solutions.json)
Use code with caution.
Bash
RL Training:

python train.py --model qwen-7b --dataset generated_solutions.json
Use code with caution.
Bash
Dataset
Automated pipeline: Problem Collection -> Gemini Flash API (initial solutions) -> Correctness Verification -> Iterative Debugging -> Data Storage (JSON).

Sample Entry:

{
  "problem": "Two Sum",
  "initial_solution": "...",
  "debug_traces": [...],
  "final_solution": "...",
  "complexity": "O(n)",
  "test_cases": [...]
}
Use code with caution.
Json
Technical Approach
Reinforcement Learning (RL):

Reward: correctness * (0.7 + 0.2*time_complexity + 0.1*space_complexity)

correctness: 1 (passes all tests) or 0.

time_complexity, space_complexity: Ratios vs. optimal (lower is better). Implementation details crucial.

Training: PPO algorithm. Iterates (max 3) or until perfect solution.

Roadmap
Phase 1: Basic Code Generation (MVP) - Completed

Phase 2: Automated Debugging - Completed

Phase 3: RL Fine-tuning - Completed

Phase 4: Performance Optimization

Phase 5: Web Interface

Repository Structure
CodeEvolveLLM/
├── main.py             # Code generation entry point
├── train.py            # RL training script
├── generate_dataset.py # Dataset generation
├── scripts/
│   └── download_model.py # LLM downloader
├── requirements.txt    # Dependencies
├── README.md           # This file
├── LICENSE             # MIT License
├── data/               # (Suggested) Datasets
├── models/             # (Suggested) Models
└── utils/              # (Suggested) Utilities
Use code with caution.
Contributing
Fork -> Branch -> Changes (clear commits) -> Pull Request.

License
MIT License. See LICENSE.
